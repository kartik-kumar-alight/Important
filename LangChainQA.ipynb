{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "975548b5-d2e7-4e35-ba33-20d4816a8cee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50760f7-87ed-42dd-9699-41b05fd8fbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import os\n",
    "# ! pip install -qq -U langchain tiktoken pypdf chromadb faiss-gpu\n",
    "# ! pip install -qq -U transformers InstructorEmbedding sentence_transformers\n",
    "# ! pip install -qq -U accelerate bitsandbytes xformers einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910213a1-151a-4b33-b2fd-114e23adb827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d32a137-a287-408a-aea2-175a2d156d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d7655b-aaf9-4b1e-89e2-8c328dd802d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install pyopenssl --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9fe1c65-a250-4d06-b8dd-1790f5f2064e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install --no-cache fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba04ac9-4689-41eb-aa64-18576c65b9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install typing-inspect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccaf2006-322f-4671-aef5-6c571bee680c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install typing_extensions==4.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541c8ce7-0173-4a73-9c7a-019b20205137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install typing_extensions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1272882-d746-473c-973c-3ed09921c4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a144f0f-298c-4466-b6dc-ecfe129a40b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install langchain --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab5f2f9-801a-4124-a59c-3f1379c3a9f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install langchain==0.0.018\n",
    "# !pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86e7789c-f435-49e3-8328-55e2ad871a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pydantic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93cffd7a-45be-4ff3-a183-55433f912d05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c012802-c906-479f-9aa6-2abd6205396f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 05:52:05.422644: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-03-04 05:52:05.422678: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.9\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import textwrap\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "# from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "import torch\n",
    "\n",
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62848f2a-924d-4f55-892a-fa41babe9d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "loader = PyPDFDirectoryLoader(\"Lean/\")\n",
    "transcript = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c929b990-fe9d-4575-a6c0-c45e58628939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('Beel_0-285593.json', 'r') as file:\n",
    "#     text_data = file.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9d5bf4-0d53-488e-9585-113a54bbe035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you have the text data as a string, and you can split it using the RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "split_texts = text_splitter.split_documents(transcript)\n",
    "# split_texts = text_splitter.create_documents(transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb9193cb-4c48-419d-a9f7-f2381c8799a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bd926ad-5e3b-425b-9cc4-3186cc8df8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Embedding and convert chunks to Vectordb using chroma##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8211e7d-be7c-4b48-84b0-dc87ba42a36c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall transformers -y\n",
    "# !pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "088cec8e-44ae-4c6e-9ad3-f596a95e6b69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ! pip install -qq -U langchain tiktoken pypdf chromadb faiss-gpu\n",
    "# ! pip install -qq -U  InstructorEmbedding sentence_transformers\n",
    "# ! pip install -qq -U accelerate bitsandbytes xformers einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebf7fc2d-858b-414d-a6d7-f9da19173900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "### this takes ~35 min to run\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "persist_directory = 'Lean_pdf'\n",
    "\n",
    "### download embeddings model\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
    "\n",
    "### create embeddings and DB\n",
    "vectordb = Chroma.from_documents(documents=split_texts,\n",
    "                                 embedding=instructor_embeddings,\n",
    "                                 persist_directory=persist_directory\n",
    "                                )\n",
    "\n",
    "\n",
    "\n",
    "### persist Chroma database\n",
    "vectordb.persist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68a12629-8754-4f42-b154-2e3cab0fc3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0bf4e73-4b6f-49cf-9cbc-a19b57f15fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "load_directory = 'Lean_pdf'\n",
    "\n",
    "### download embeddings model\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\"\n",
    "                                                     )\n",
    "\n",
    "vectordb = Chroma(persist_directory=load_directory, embedding_function=instructor_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf42d6d9-fe71-4b9e-a1e6-dcf63bff7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use falcon LLM model or any other model##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "009127a6-f444-4f11-b484-598c1fe62734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a349988-f8fc-4c94-8c65-8a2585b69cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall torch transformers -y\n",
    "# !pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73fc5035-db37-4c08-a1a4-b8e56dfd256b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65e0ea73-138e-486c-bd15-9ed5294b06f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2530c34f-18e6-4a2f-a270-ba0b4b2ecb1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b7a62a28714ecd9ed5d4af6e951d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "# tensor = torch.empty(shape)\n",
    "# device = torch.device(\"cpu\")\n",
    "from peft import PeftModel \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "# from transformers.models.bert.modeling_bert import BertModel,BertForMaskedLM\n",
    "# quantization_config = BitsAndBytesConfig(llm_int8_threshold=200.0)\n",
    "tokenizer = AutoTokenizer.from_pretrained('daryl149/llama-2-13b-chat-hf')\n",
    " # daryl149/llama-2-13b-chat-hf       \n",
    "model = AutoModelForCausalLM.from_pretrained(\"daryl149/llama-2-13b-chat-hf\",\n",
    "                                             # load_in_4bit=True,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             # trust_remote_code=True,\n",
    "                                             # quantization_config=quantization_config,\n",
    "                                            )\n",
    "max_len = 4096\n",
    "task = \"text-generation\"\n",
    "T = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54ebd74b-a4d1-49ea-afad-da581b7f42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pass llm model into the huggingface Pipeline###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7d3fa51-24a6-4264-9d40-47d338a782f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task=task,\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=max_len,\n",
    "    temperature=T,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e5e3abd-d462-4861-9897-9439b8c85259",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pass full pipeline into the QA chain####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "017313bd-e680-48a4-81f1-3e582a432f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "# _template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a \n",
    "# standalone question without changing the content in given question.\n",
    "\n",
    "# Chat History:\n",
    "# {chat_history}\n",
    "# Follow Up Input: {question}\n",
    "# Standalone question:\"\"\"\n",
    "# condense_question_prompt_template = PromptTemplate.from_template(_template)\n",
    "\n",
    "# prompt_template = \"\"\"You are helpful information giving QA System and make sure you don't answer anything \n",
    "# not related to following context. You are always provide useful information & details available in the given context. Use the following pieces of context to answer the question at the end. \n",
    "# If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# Helpful Answer:\"\"\"\n",
    "# Define prompt template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use two sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# qa_prompt = PromptTemplate(\n",
    "#     template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fa405e6-2521-48f5-8f33-45348f327aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from functools import lru_cache\n",
    "\n",
    "# Initialize a ConversationalMemory instance\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "question_generator = LLMChain(llm=llm, prompt=qa_prompt, memory=memory)\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "qa_chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    # return_source_documents=True,\n",
    "    verbose=True,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# # Define a function to cache retriever results\n",
    "# @lru_cache(maxsize=None)  # None indicates unlimited cache size\n",
    "# def cached_retriever_result(question):\n",
    "#     return qa_chain({'question': question, 'chat_history': chat_history})\n",
    "\n",
    "# # Rest of your code\n",
    "# chat_history = []\n",
    "# while True:\n",
    "#     question = input(\"You: \")  # Get user input\n",
    "\n",
    "#     # Query the cached retriever function\n",
    "#     result = cached_retriever_result(question)\n",
    "\n",
    "#     response = result['answer']\n",
    "#     chat_history.append((question, response))\n",
    "\n",
    "#     print(\"AI:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a50fb928-06f8-4f65-9820-14a7b9b35b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': ['what is Six Sigma?'], 'answer': [' Six Sigma is a methodology for improving the efficiency and effectiveness of business processes. It uses statistical tools and techniques to identify and eliminate defects, variations, and waste in order to achieve near-perfect quality levels.'], 'contexts': [['Institute for Innovation\\nand Improvement\\nLean Six Sigma: \\nsome basic concepts', 'Institute for Innovation\\nand Improvement\\nLean Six Sigma: \\nsome basic concepts', 'Institute for Innovation\\nand Improvement\\nLean Six Sigma: \\nsome basic concepts']], 'ground_truth': ['Six Sigma is a process improvement methodology developed at Motorola in the1980’s to reduce defects in its processes. Its goal was to achieve a level ofperformance equal to a defect rate of 3.4 defects per million opportunities – this is avirtually defect free environment i.e. Six Sigma performance']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [\"what is Six Sigma?\"]\n",
    "ground_truths = [\"Six Sigma is a process improvement methodology developed at Motorola in the1980’s to reduce defects in its processes. Its goal was to achieve a level ofperformance equal to a defect rate of 3.4 defects per million opportunities – this is avirtually defect free environment i.e. Six Sigma performance\"]\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Inference\n",
    "for question in questions:\n",
    "    answer = qa_chain.invoke(question)\n",
    "    answer = answer.get('answer')\n",
    "    context_list = [docs.page_content for docs in retriever.get_relevant_documents(question)]\n",
    "    \n",
    "    answers.append(answer)\n",
    "    contexts.append(context_list)\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truths\n",
    "}\n",
    "print(data)\n",
    "# # Convert dict to dataset\n",
    "# dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "90cda3b7-b5b1-4c1a-b587-fd98d3bf015d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "1b6d6ebc-28dd-4577-a37c-8e19576b5cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [\"What is Six Sigma?\",\n",
    "             \"What is Six Sigma?\"]\n",
    "ground_truths = [\"Six Sigma is a process improvement methodology developed at Motorola in the1980’s to reduce defects in its processes. Its goal was to achieve a level ofperformance equal to a defect rate of 3.4 defects per million opportunities – this is avirtually defect free environment i.e. Six Sigma performance\",\n",
    "                \"Innovation\\nand Improvement\\nLean Six Sigma: Six Sigma focuses primarily on reducing variation, whilst Lean focuses on improving flow in the value\"]\n",
    "\n",
    "answers = [\"Six Sigma is a methodology for improving the efficiency and effectiveness of business processes. It uses statistical tools and techniques to identify and eliminate defects, variations, and waste in order to achieve near-perfect quality levels.\",\n",
    "           \"Lean techniques, such as value stream mapping, have been used in the NHS since the mid 1990s, largely through episodic Kaizen events or by combining Lean tools withother improvement approaches. \"]\n",
    "contexts = [[\"'Six Sigma is a methodology efficiency developed at Motorola in .Institute for Innovation\\nand Improvement\\nLean Six Sigma: \\nsome basic concepts', 'Institute for Innovation\\nand Improvement\\nLean Six Sigma: \\nsome basic concepts', 'Institute for Innovation\\nand Improvement\\nLean Six Sigma: \\nsome basic concepts'\"],\n",
    "            [\"Institute Lean Six Sigma,Six Sigma focuses primarily\"]]\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\":ground_truths\n",
    "}\n",
    "# print(data)\n",
    "# # Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "283db9ad-19b3-4f84-b03e-226fde9b1951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = Dataset.from_dict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "f5e25637-2e1f-4fa3-8fa6-7a2808ed4f78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "756a90ca-627d-488f-8105-bbadc8241723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "76f8285a-c3e7-497b-ba06-2e4d0da4f7d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f1083450-ac43-4cb3-b605-498d6542603d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2eeef4db464fb6b310ccf6a5d1c43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    # faithfulness,\n",
    "    # answer_relevancy,\n",
    "    context_recall,\n",
    "    # context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[\n",
    "        # context_precision,\n",
    "        context_recall,\n",
    "        # faithfulness,\n",
    "        # answer_relevancy,\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3b03318d-4652-4fdd-8f1a-ff63f4591d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Six Sigma?</td>\n",
       "      <td>Six Sigma is a methodology for improving the e...</td>\n",
       "      <td>['Six Sigma is a methodology efficiency develo...</td>\n",
       "      <td>Six Sigma is a process improvement methodology...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Six Sigma?</td>\n",
       "      <td>Lean techniques, such as value stream mapping,...</td>\n",
       "      <td>[Institute Lean Six Sigma,Six Sigma focuses pr...</td>\n",
       "      <td>Innovation\\nand Improvement\\nLean Six Sigma: S...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             question                                             answer  \\\n",
       "0  What is Six Sigma?  Six Sigma is a methodology for improving the e...   \n",
       "1  What is Six Sigma?  Lean techniques, such as value stream mapping,...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  ['Six Sigma is a methodology efficiency develo...   \n",
       "1  [Institute Lean Six Sigma,Six Sigma focuses pr...   \n",
       "\n",
       "                                        ground_truth  context_recall  \n",
       "0  Six Sigma is a process improvement methodology...             1.0  \n",
       "1  Innovation\\nand Improvement\\nLean Six Sigma: S...             1.0  "
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "fac93bdf-3760-4e21-8e46-7852384ae06e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58532d4-71e0-499b-b430-fb1699543367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d2351-a290-4406-85be-889f06873ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a0d509-2e40-4cbc-a9da-1f4ab247f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def final_text(text, width=160000):\n",
    "    lines = text.split('\\n')\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "    return wrapped_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da69bd-faad-4588-a99a-fdf8512ea543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_llm_response(llm_final_response):\n",
    "    print(final_text(llm_final_response['result']))\n",
    "\n",
    "def final_answer(question):\n",
    "    llm_final_response = qa_chain(question)\n",
    "    answer = final_llm_response(llm_final_response)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981131a6-6ba9-46ff-a4ef-677d889e3f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# question = \"A farm house uses atleast 800 kg of special food daily. The special food is a mixture of corn and soyabean with the following compositions\"\n",
    "# final_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334189d-cf71-41be-ad4a-0fe81447d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"what is six sigma?\"\n",
    "final_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47720360-f5ee-4874-8950-b902d96dac29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54444a-c24a-48d4-a0a5-68d122617d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f4d11-3517-4de6-9bba-96619c5dcf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0808bf3-4a1c-4229-8932-6d2781623fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (adl-core-sagemaker-image/6)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:110008799848:image-version/adl-core-sagemaker-image/6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
